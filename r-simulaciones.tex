\section{Interfaz}

\subsection{Prueba en tiempo real}
Para las primeras pruebas se utilizaron los 4 modelos exportados, la cámara integrada y un bucle infinito. En este caso es utilizaron las siguientes librerías \textit{CV2}, \textit{TensorFlow} y \textit{Numpy}. El primer paso de la configuración fue cargar el modelo previamente exportado. Luego, se realiza la inicialización de la cámara integrada por medio de \textit{OpenCV}, y se ajusta el tamaño de la imagen a obtener por medio de comandos de la librería \textit{CV2}. 
Ahora bien, en el bucle infinito se realiza una captura de la imagen obtenida por la cámara, la cual se guarda como una matrices de píxeles. Luego, se cambia el canal de BGR a RGB, se normaliza la matriz al dividirla entre 255, y se cambia la matriz de tipo flotante a entero. Después, por medio de la librería \textit{TensorFlow} se utiliza la función Predecir, la cual devuelve una vector con el porcentaje de similitud predecido para cada clase. Finalmente, se aplica la función de \textit{Numpy} para obtener el valor más máximo dentro de dicho vector de predicciones y dicho valor se imprime en la consola inicialmente.

\subsection{Bloques de funciones}
La primera versión de la interfaz creada, utiliza 3 grandes grupos de códigos. El primer grupo se encarga de capturar fotos y clasificarlas según la etiqueta que se necesite utilizar, en este caso se trabaja con 4. En esta ventada se presentan instrucciones en forma de lista para que el usuario pueda identificar y capturar las fotografías, por medio de una cuadrícula de selección. Luego, el segundo bloque cuenta con las opciones necesarias para preparar las imágenes de tal manera que puedan ser interpretadas por los modelos de programación. Por último, está el bloque de prueba en vivo, esta utiliza el código descrito en la sección ``Prueba en tiempo real''. Además, es importante resaltar que todos estos bloques funcionan con códigos los códigos utilizando en el capítulo ``Algoritmos'' dado que estos fueron construidos con programación orientada a objetos. Por último, para la creación de esta interfaz se utilizaron las librerías \textit{Tkinter} y \textit{CustomTkinter} respectivamente. 

\section{Consola (Primer grupo de modelos)}
En el caso de los primeros modelos realizados, la experiencia fue pésima, dado que el modelo predecía en su mayoría las posiciones inferiores, tanto para los modelos con 9 y 6 clases. Se intentó realizar dicho proceso de movimiento a diferentes distancias de la cámara pero el resultado fue el mismo. Para visualizar los resultados de estos modelos, se utilizó el código de prueba en tiempo real, imprimiendo valor de la predicción del modelo en la consola, como estaba previsto inicialmente

\section{Turtle (Segundo grupo de modelos)}
En cuanto a los modelos de 4 y 3 clases, el resultado depende de la distancia de separación del rostro del usuario y de la cámara. A más de 30 cm de distancia, las predicciones son bastante malas, dado que muchas veces se la predicción es errónea en cuanto al lado de la orientación de la cabeza. Sin embargo, al realizar pruebas justo a 30 cm de la cámara, tanto para el modelo de 3 y 4 clases, el resultado fue satisfactorio. Esto puede indicar que falta entrenar al modelo con diferentes tipos de fondos, dado que cuando la cámara capta en su mayoría el rostro de la persona, las predicciones son buenas. Además, se realizó la prueba a más de 30 cm en diferentes espacios de trabajo en la casa de Gerardo Fuentes, y los resultados no fueron buenos, hasta que se realizó el acercamiento nuevamente. En este caso, ya se contaba con la primera versión de la interfaz que es capaz de capturar y manipular imágenes, así como graficar por medio de la librería \textit{Turtle} de \textit{Python} el movimiento de un cursor, así como su trayectoria. 
 
\section{Webots (Tercer grupo de modelos)}
En esta simulación fue necesario utilizar el software \textit{Webots}, crear un mundo simple, que contuviera un piso rectangular y el robot a utilizar. En este caso se optó por el modelo del robot \textit{E-puck}, dado que es un agente robótico móvil de dos ruedas, muy similar a una silla de ruedas. Además, se configuró el robot para utilizar un controlador externo, en vez de la opción por defecto, que permite utilizar un archivo incluido en la interfaz del software. La selección de un controlador externo fue realizada, dado que se requiere utilizar tanto las librerías del robot, como las que se han implementado para el manejo de datos y la predicción de clases en tiempo real. También es importante resaltar, que el código utilizado para manejar la simulación en \textit{Webots}, aún no se ha incorporado a la interfaz. 

En estas simulaciones, el séptimo y octavo modelos funcionaron de manera similar. En esta ocasión, la predicción de las posiciones funcionaba en un rango mayor de distancias, sin embargo existía una tendencia a predecir posiciones centrales de la cabeza como rotaciones a la derecha. Sin embargo, este último problema parecía no aparecer cuando el usuario se posicionaba con un desfase respecto de la cámara hacia la izquierda. Por último, el noveno modelo funciona en un mayor rango de posiciones, distancias y fondos, aunque requiere utilizar los marcadores faciales, incluso así, llegando a presentar los problemas mencionados con el séptimo y octavo modelos, pero con menor error. Además, estos resultados fueron obtenidos al ser Gerardo Fuentes (autor) el usuario, dado que con usuarios nuevos, el resultado aún no es óptimo. Todo esto se consiguió, con una exactitud menor al 70\% que se observó en las gráficas de dicho modelo en la sección de `` Tercer grupo de modelos de aprendizaje'' en el capítulo ``Algoritmos''.
